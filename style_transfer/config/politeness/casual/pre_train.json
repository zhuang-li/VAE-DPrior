{
    "data_type": "politeness",
    "learning_rate": 0.001,
    "batch_size": 50,
    "cache_path": "lambda_model/",
    "gradient_accumulation_steps":1,
    "num_clusters": 10,
    "use_gpu": true,
    "prefix_dir": "../../PrefixTuning",
    "prefix_model_name": "politeness_pretrain_3200_topics_150_epoch",
    "prefix_task_mode": "proto_reg",
    "vae": "ivae",
    "label_name": "../../dataset/politeness/label_names.txt",
    "data_dir": "../../dataset/politeness/",
    "topic_prior": false,
    "aug_epoch":150,
    "is_finetune": false,
    "aug_iter": 1800,
    "topic_coeff":0.1,
    "mse_coeff":1,
    "decay":0.99,
    "epsilon":1e-5,
    "pretrain_coeff":60,
    "sample_per_class": 80,
    "classes_per_episode": 3,
    "n_support": 4,
    "max_length": 64,
    "warm_epoch": 5,
    "re_cluster": 1000,
    "reg_cluster": 100,
    "attention": "dot",
    "topic_num": 3200,
    "preseqlen": 100,
    "style_transfer_model":"casual",
    "tuning_mode": "prefixtune",
    "bert": "prajjwal1/bert-small",
    "gpt2_model_name": "gpt2",
    "disentangle_loss": 3,
    "vae_type": 0,
    "intervention": false,
    "gaussian":true,
    "gas_coeff":0.1,
    "activation": "tanh",
    "is_hard_ivae":false
}